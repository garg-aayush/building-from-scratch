{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8778b2c2-9f85-4dc2-a14a-a5573e38c879",
   "metadata": {},
   "source": [
    "## Mathematical trick in self-attention\n",
    "\n",
    "Below is a mathematical trick that's at the heart of efficient self-attention implementation in Transformers. We will use the below toy example toy example to understand this operation.\n",
    "\n",
    "Currently, the 8 tokens in each batch don't communicate with each other. We want them to **\"talk\"** to each other, but with a specific constraint: each token should only communicate with previous tokens, not future ones. For example, the token at position 5 should only access information from positions 1, 2, 3, 4, and 5 - never from positions 6, 7, or 8, since those represent future information we're trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a27889-feff-43bd-9ad5-447c2293810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec42f55-7838-4995-84a8-0241f2043a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,2 # batch, seq_length, vocab_size\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c7552-c9c9-4847-80db-dc440cbbe84d",
   "metadata": {},
   "source": [
    "**The simplest way for tokens to communicate is through averaging.** If I'm the 5th token, I can take my channels and average them with the channels from all previous positions (1st through 4th). This creates a feature vector that summarizes me in the context of my history.\n",
    "\n",
    "While averaging is a weak form of interaction that loses spatial arrangement information, it's a good starting point. We'll see how to add that information back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15833463-dbd9-4d3b-98bc-29adbfb767f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros(x.shape)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "x[0], xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302faf51-64f4-48e4-8598-63c99bae2493",
   "metadata": {},
   "source": [
    "We can make this process highly efficient using matrix multiplication. Let demonstrate it with a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d9291-b0d7-49c6-9630-f5182b047538",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones((3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5810444-7887-4f07-919e-9c4d5790eb5c",
   "metadata": {},
   "source": [
    "**key trick**: Instead of using a boring matrix of all ones, PyTorch provides a function called `tril()` wrapped in `torch.ones()`. This returns only the lower triangular portion of the matrix, zeroing out the upper elements. \n",
    "\n",
    "This creates an incremental aggregation pattern where each position accumulates information from all previous positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99609575-2f3b-4f5f-ac41-de7f96fc9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# create lower triangular matrix\n",
    "a = torch.tril((torch.ones((3,3))))\n",
    "# avg it along 1st dimension\n",
    "a = a / a.sum(1,keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643543b5-9704-4e3d-aef6-00649330b9b1",
   "metadata": {},
   "source": [
    "This can be used to achieve averaging along the sequence_length efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4330bb-9c56-4d66-80b7-101a79e62cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "w = torch.tril(torch.ones(T,T))\n",
    "w = w / w.sum(1, keepdim=True)\n",
    "xbow2 = w @ x # (T,T) @ (B,T,C) -> (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2291360-6ff4-4d4a-9dcf-e515909543ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(xbow2, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dadfab-bf93-44ad-8fdb-e477dcf29839",
   "metadata": {},
   "source": [
    "Now We will rewrite this in a third way using `softmax`. Softmax is a normalization operation. When we exponentiate each element:\n",
    "- The finite values (where `tril` was 1) become e^0 = 1\n",
    "- The negative infinity values (where `tril` was 0) become e^(-inf) = 0\n",
    "\n",
    "Note: Currently these affinities are just set to zero by us, but in self-attention, these affinities won't be constant. They'll be data-dependent. Tokens will look at each other and find some tokens more or less interesting based on their content. The affinities will vary depending on how much tokens \"want\" to attend to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4b361-60d2-4811-bb79-7c1f2326d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use softmax\n",
    "from torch.nn import functional as F\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "w = torch.zeros((T,T))\n",
    "w = w.masked_fill(tril == 0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "xbow3 = w @ x\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c5f7b-92b8-4567-8896-51041278a9a1",
   "metadata": {},
   "source": [
    "This is the preview for self-attention. The key takeaway from this entire section is that you can perform wghted aggregations of past elements using matrix multiplication in a lower triangular fashion. The elements in the lower triangular part determine how much each past element contributes to the current position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a03224-f810-4d24-a0a1-13e485a9860e",
   "metadata": {},
   "source": [
    "**Crux of self-attention**\n",
    "\n",
    "- It is probably the most important part of transformers to understand. _**So, please spend as much time as possible to understand this.**_\n",
    "- The above version does a simple average of all the past tokens and the current token. So the previous information and current information is just being mixed together in an average.\n",
    "\n",
    "**Making Attention Data-Dependent**\n",
    "- Now we don't actually want this to be all uniform because different tokens will find different other tokens more or less interesting, and we want that to be data-dependent.\n",
    "- For example, if I'm a vowel, then maybe I'm looking for consonants in my past, and maybe I want to know what those consonants are and I want that information to flow to me. So I want to now gather information from the past, but I want to do it in a data-dependent way. **This is the problem that self-attention solves**.\n",
    "\n",
    "**How self-attention makes makes interactions/affinities data-dependent?**\n",
    "- The way self-attention solves this is the following: every single node/token at each position will emit two vectors - it will emit a **query** and it will emit a **key**.\n",
    "    - The **query** vector roughly speaking is \"what am I looking for?\"\n",
    "    - The **key** vector roughly speaking is \"what do I contain?\"\n",
    "- Then the way we get affinities between these tokens in a sequence is we basically just do a dot product between the keys and the queries.\n",
    "- So my query dot products with all the keys of all the other tokens, and that dot product now becomes `w`. So if the key and the query are sort of aligned, they will interact to a very high amount (and vica-versa), and then I will get to learn more about that specific token as opposed to any other token in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065179a1-9b21-4797-9114-3fa3f43744b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4: self-attention (for single head)\n",
    "from torch import nn\n",
    "from einops import einsum\n",
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,head_size)\n",
    "q = query(x)# (B,T,head_size)\n",
    "# w = q @ k.transpose(-2,-1)\n",
    "w = einsum(q,k, 'B T1 C, B T2 C -> B T1 T2') # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "w = w.masked_fill(tril == 0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "out = w @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c8887-21da-41ea-9c28-78a9ea2b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w is not uniform anymore\n",
    "w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62749ed5-7920-41a2-b9a4-a5680009a051",
   "metadata": {},
   "source": [
    "**Adding value**\n",
    "\n",
    "- Now there's one more part to a single self-attention head, and that is that when we do the aggregation, we don't actually aggregate the tokens(`x`) exactly. We produce one more value here and we call that the **value**.\n",
    "- So `v` is the elements that we aggregate instead of the raw `x`. You can think of `x` as kind of like private information to this token. So X is kind of private to this token - I'm the fifth token at some position and I have some identity, and my information is kept in vector `x`. Now for the purposes of the single head: here's what I'm interested in, here's what I have, and if you find me interesting, here's **what I will communicate to you** - and that's stored in `v`. `v` is the thing that gets aggregated for the purposes of this single head between the different nodes.\n",
    "- This is basically the self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a2c6c-6be6-4d1c-839d-1e8320897c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from einops import einsum\n",
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,head_size)\n",
    "q = query(x)# (B,T,head_size)\n",
    "v = value(x) # (B,T,head_size)\n",
    "# w = q @ k.transpose(-2,-1)\n",
    "# w = einsum(q,k, 'B T1 C, B T2 C -> B T1 T2') # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "w = torch.zeros((T,T))\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "w = w.masked_fill(tril == 0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "out = w @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ea94a-8fb3-4d68-a943-e6d2cc50d929",
   "metadata": {},
   "source": [
    "![directed-graph](images/directed-graph.webp)\n",
    "- Attention is a **communication mechanism**\n",
    "  - You can really think about it as a communication mechanism where you have a number of nodes in a directed graph where basically you have edges pointed between nodes like this. What happens is every node has some vector of information, and it gets to aggregate information via a weighted sum from all of the nodes that point to it.\n",
    "- **There is no notion of space**.\n",
    "  - Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "  - Notice that there's no inherent notion of space in attention. It simply acts over a set of vectors, and by default, these nodes have no idea about their positions. That's why we need positional encodings - to give the nodes information about where they are located in the sequence. This is different from convolutions, which have a very specific spatial layout and operate directly in that space.\n",
    "- **Each example across batch dimension is of course processed completely independently** and never \"talk\" to each other\n",
    "  - Elements across the batch dimension never communicate with each other - they're processed completely independently. This is achieved through batched matrix multiplication that applies the same operation in parallel across the batch dimension. So in our analogy of a directed graph, with a batch size of 4, we actually have four separate pools of 8 nodes each. The 8 nodes within each pool can communicate, but the different pools never interact.\n",
    "- **Encoder vs Decoder Block**\n",
    "  - This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "  - However, in many other applications like sentiment analysis, you might want all tokens to communicate with each other fully. In those cases, you'd use an \"encoder block\" - which is simply attention without the triangular mask (delete the single line that does masking with `tril`), allowing all nodes to talk to each other completely.\n",
    "- **Self-attention vs Cross-attention**\n",
    "  - self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- **Scaled attention** additional divides `w` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, w will be unit variance too.\n",
    "  - This scaling is crucial because the attention weights feed into a softmax function. If the attention weights become too large (high variance), the softmax will become very \"peaky\" - it will converge toward one-hot vectors where each token only attends to a single other token. This is especially problematic at initialization, where we want the attention to be fairly diffuse so tokens can learn to attend to multiple relevant positions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94415ae-cb28-47f1-b324-a990b1ef9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "w = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "print(\"k.var()\", k.var())\n",
    "print(\"q.var()\", q.var())\n",
    "print(\"w.var()\", w.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e9913-fae0-408c-9983-612da0360be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e76200-1f65-47be-9089-be1fe852d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40794ba0-92c5-493f-8ec3-3a62cc29dd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
