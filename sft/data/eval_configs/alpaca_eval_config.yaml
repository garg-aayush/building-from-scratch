# AlpacaEval Evaluation Configuration

evaluator: AlpacaEval

evaluator_params:
  data_file: data/eval_data/alpaca_eval/alpaca_eval.jsonl
  system_prompt_file: data/zero_shot_system_prompt.prompt
  generator_name: llama-3.1-8b-base
  reference_outputs_file: data/eval_data/alpaca_eval/alpaca_eval_gpt4_baseline.json
  annotators_config_dir: data/eval_data/alpaca_eval/configs
  results_dir: results/instruct/baseline
  eval_filename: baseline_alpaca_eval_outputs.json
  seed: 1337

model_name: meta-llama/Llama-3.1-8B

sampling_params:
  temperature: 0.0
  top_p: 1.0
  max_tokens: 1024
  stop:
    - "```"

model_params:
  max_model_len: 2048
  max_num_seqs: 32

save_results: true
