{
  "world religions": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.8011695906432749,
    "num_examples": 171
  },
  "virology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5060240963855421,
    "num_examples": 166
  },
  "us foreign policy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.81,
    "num_examples": 100
  },
  "sociology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.835820895522388,
    "num_examples": 201
  },
  "security studies": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6530612244897959,
    "num_examples": 245
  },
  "public relations": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5636363636363636,
    "num_examples": 110
  },
  "professional psychology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6339869281045751,
    "num_examples": 612
  },
  "professional medicine": {
    "parsed_accuracy": 0.9191176470588235,
    "accuracy": 0.5845588235294118,
    "num_examples": 272
  },
  "professional law": {
    "parsed_accuracy": 0.9947848761408083,
    "accuracy": 0.44589308996088656,
    "num_examples": 1534
  },
  "professional accounting": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.475177304964539,
    "num_examples": 282
  },
  "prehistory": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6604938271604939,
    "num_examples": 324
  },
  "philosophy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6527331189710611,
    "num_examples": 311
  },
  "nutrition": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6535947712418301,
    "num_examples": 306
  },
  "moral scenarios": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.25251396648044694,
    "num_examples": 895
  },
  "moral disputes": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6329479768786127,
    "num_examples": 346
  },
  "miscellaneous": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7854406130268199,
    "num_examples": 783
  },
  "medical genetics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.69,
    "num_examples": 100
  },
  "marketing": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.8547008547008547,
    "num_examples": 234
  },
  "management": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7766990291262136,
    "num_examples": 103
  },
  "machine learning": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.36607142857142855,
    "num_examples": 112
  },
  "logical fallacies": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.754601226993865,
    "num_examples": 163
  },
  "jurisprudence": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6666666666666666,
    "num_examples": 108
  },
  "international law": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6942148760330579,
    "num_examples": 121
  },
  "human sexuality": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7175572519083969,
    "num_examples": 131
  },
  "human aging": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6322869955156951,
    "num_examples": 223
  },
  "high school world history": {
    "parsed_accuracy": 0.9957805907172996,
    "accuracy": 0.7848101265822784,
    "num_examples": 237
  },
  "high school us history": {
    "parsed_accuracy": 0.9803921568627451,
    "accuracy": 0.75,
    "num_examples": 204
  },
  "high school statistics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.4351851851851852,
    "num_examples": 216
  },
  "high school psychology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7779816513761468,
    "num_examples": 545
  },
  "high school physics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.3841059602649007,
    "num_examples": 151
  },
  "high school microeconomics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6092436974789915,
    "num_examples": 238
  },
  "high school mathematics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.32592592592592595,
    "num_examples": 270
  },
  "high school macroeconomics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5435897435897435,
    "num_examples": 390
  },
  "high school government and politics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.8031088082901554,
    "num_examples": 193
  },
  "high school geography": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7171717171717171,
    "num_examples": 198
  },
  "high school european history": {
    "parsed_accuracy": 0.9818181818181818,
    "accuracy": 0.7454545454545455,
    "num_examples": 165
  },
  "high school computer science": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.58,
    "num_examples": 100
  },
  "high school chemistry": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.4729064039408867,
    "num_examples": 203
  },
  "high school biology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6935483870967742,
    "num_examples": 310
  },
  "global facts": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.39,
    "num_examples": 100
  },
  "formal logic": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.3492063492063492,
    "num_examples": 126
  },
  "elementary mathematics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.43386243386243384,
    "num_examples": 378
  },
  "electrical engineering": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5655172413793104,
    "num_examples": 145
  },
  "econometrics": {
    "parsed_accuracy": 0.9385964912280702,
    "accuracy": 0.45614035087719296,
    "num_examples": 114
  },
  "conceptual physics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5319148936170213,
    "num_examples": 235
  },
  "computer security": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.73,
    "num_examples": 100
  },
  "college physics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.4019607843137255,
    "num_examples": 102
  },
  "college medicine": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5664739884393064,
    "num_examples": 173
  },
  "college mathematics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.3,
    "num_examples": 100
  },
  "college computer science": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.53,
    "num_examples": 100
  },
  "college chemistry": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.39,
    "num_examples": 100
  },
  "college biology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6875,
    "num_examples": 144
  },
  "clinical knowledge": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6679245283018868,
    "num_examples": 265
  },
  "business ethics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6,
    "num_examples": 100
  },
  "astronomy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6052631578947368,
    "num_examples": 152
  },
  "anatomy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5851851851851851,
    "num_examples": 135
  },
  "abstract algebra": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.36,
    "num_examples": 100
  },
  "all_subjects": {
    "accuracy": 0.5818259507192708,
    "parsed_accuracy": 0.9967953283008119,
    "num_examples": 14042
  }
}