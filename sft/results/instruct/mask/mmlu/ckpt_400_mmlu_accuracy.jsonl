{
  "world religions": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.8011695906432749,
    "num_examples": 171
  },
  "virology": {
    "parsed_accuracy": 0.9939759036144579,
    "accuracy": 0.4819277108433735,
    "num_examples": 166
  },
  "us foreign policy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.82,
    "num_examples": 100
  },
  "sociology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.8656716417910447,
    "num_examples": 201
  },
  "security studies": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6775510204081633,
    "num_examples": 245
  },
  "public relations": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6272727272727273,
    "num_examples": 110
  },
  "professional psychology": {
    "parsed_accuracy": 0.9950980392156863,
    "accuracy": 0.6437908496732027,
    "num_examples": 612
  },
  "professional medicine": {
    "parsed_accuracy": 0.9963235294117647,
    "accuracy": 0.6397058823529411,
    "num_examples": 272
  },
  "professional law": {
    "parsed_accuracy": 0.9915254237288136,
    "accuracy": 0.4335071707953064,
    "num_examples": 1534
  },
  "professional accounting": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.4858156028368794,
    "num_examples": 282
  },
  "prehistory": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6759259259259259,
    "num_examples": 324
  },
  "philosophy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6816720257234726,
    "num_examples": 311
  },
  "nutrition": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6830065359477124,
    "num_examples": 306
  },
  "moral scenarios": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.2636871508379888,
    "num_examples": 895
  },
  "moral disputes": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6445086705202312,
    "num_examples": 346
  },
  "miscellaneous": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7905491698595147,
    "num_examples": 783
  },
  "medical genetics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7,
    "num_examples": 100
  },
  "marketing": {
    "parsed_accuracy": 0.9957264957264957,
    "accuracy": 0.8461538461538461,
    "num_examples": 234
  },
  "management": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7572815533980582,
    "num_examples": 103
  },
  "machine learning": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.4017857142857143,
    "num_examples": 112
  },
  "logical fallacies": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7239263803680982,
    "num_examples": 163
  },
  "jurisprudence": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6851851851851852,
    "num_examples": 108
  },
  "international law": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7107438016528925,
    "num_examples": 121
  },
  "human sexuality": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.732824427480916,
    "num_examples": 131
  },
  "human aging": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6143497757847534,
    "num_examples": 223
  },
  "high school world history": {
    "parsed_accuracy": 0.9662447257383966,
    "accuracy": 0.759493670886076,
    "num_examples": 237
  },
  "high school us history": {
    "parsed_accuracy": 0.9313725490196079,
    "accuracy": 0.7058823529411765,
    "num_examples": 204
  },
  "high school statistics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.4722222222222222,
    "num_examples": 216
  },
  "high school psychology": {
    "parsed_accuracy": 0.998165137614679,
    "accuracy": 0.7871559633027523,
    "num_examples": 545
  },
  "high school physics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.423841059602649,
    "num_examples": 151
  },
  "high school microeconomics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6512605042016807,
    "num_examples": 238
  },
  "high school mathematics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.3333333333333333,
    "num_examples": 270
  },
  "high school macroeconomics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5769230769230769,
    "num_examples": 390
  },
  "high school government and politics": {
    "parsed_accuracy": 0.9948186528497409,
    "accuracy": 0.7979274611398963,
    "num_examples": 193
  },
  "high school geography": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7171717171717171,
    "num_examples": 198
  },
  "high school european history": {
    "parsed_accuracy": 0.9636363636363636,
    "accuracy": 0.7212121212121212,
    "num_examples": 165
  },
  "high school computer science": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.62,
    "num_examples": 100
  },
  "high school chemistry": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5024630541871922,
    "num_examples": 203
  },
  "high school biology": {
    "parsed_accuracy": 0.9967741935483871,
    "accuracy": 0.6967741935483871,
    "num_examples": 310
  },
  "global facts": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.38,
    "num_examples": 100
  },
  "formal logic": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.3412698412698413,
    "num_examples": 126
  },
  "elementary mathematics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.42857142857142855,
    "num_examples": 378
  },
  "electrical engineering": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5379310344827586,
    "num_examples": 145
  },
  "econometrics": {
    "parsed_accuracy": 0.9736842105263158,
    "accuracy": 0.47368421052631576,
    "num_examples": 114
  },
  "conceptual physics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5063829787234042,
    "num_examples": 235
  },
  "computer security": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7,
    "num_examples": 100
  },
  "college physics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.39215686274509803,
    "num_examples": 102
  },
  "college medicine": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.5838150289017341,
    "num_examples": 173
  },
  "college mathematics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.29,
    "num_examples": 100
  },
  "college computer science": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.44,
    "num_examples": 100
  },
  "college chemistry": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.37,
    "num_examples": 100
  },
  "college biology": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.7013888888888888,
    "num_examples": 144
  },
  "clinical knowledge": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6792452830188679,
    "num_examples": 265
  },
  "business ethics": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.59,
    "num_examples": 100
  },
  "astronomy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6644736842105263,
    "num_examples": 152
  },
  "anatomy": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.6444444444444445,
    "num_examples": 135
  },
  "abstract algebra": {
    "parsed_accuracy": 1.0,
    "accuracy": 0.36,
    "num_examples": 100
  },
  "all_subjects": {
    "accuracy": 0.5883065090442957,
    "parsed_accuracy": 0.9962256088876228,
    "num_examples": 14042
  }
}