{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-10",
      "metadata": {},
      "source": [
        "# Bigram Language Model (from scratch)\n",
        "\n",
        "In this notebook, Build and train a tiny character-level bigram language model from [Andrej Karpathy’s \"Let’s build GPT from scratch\"](https://www.youtube.com/watch?v=kCc8FmEb1nY). Keep things simple and self-contained: load Tiny Shakespeare, tokenize at the character level, train a bigram model (each token predicts the next), and sample text.\n",
        "\n",
        "- It is best to follow this notebook with the Andrej's video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-20",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Tiny Shakespeare once (toy dataset)\n",
        "# Shakespeare - a concatenation of all of Shakespeare's works in a single file.\n",
        "\n",
        "!mkdir -p data && \\\n",
        "    wget -O ../data/input.txt \\\n",
        "    https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-30",
      "metadata": {},
      "source": [
        "## Load Text\n",
        "Read the entire corpus into memory. It’s small (about 1.1M chars), which is perfect for a quick toy experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the dataset\n",
        "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "print(f\"Text length: {len(text)}\")\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocab-50",
      "metadata": {},
      "source": [
        "## Vocabulary and Tokenization\n",
        "Use a simple character-level tokenizer. That means each unique character becomes a token id. It’s crude compared to BPE/sentencepiece, but perfect for understanding the mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b18d006",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(\"vocab_size =\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocab-60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-level tokenizer: char <-> id\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # string -> list[int]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])  # list[int] -> string\n",
        "\n",
        "print(encode(\"Hello, there!\"))\n",
        "print(decode(encode(\"Hello, there!\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numericalize-70",
      "metadata": {},
      "source": [
        "## Numericalize and Split\n",
        "Convert the full corpus to token ids (a long 1D tensor), then keep 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1716ab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numericalize-80",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/val split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "context-90",
      "metadata": {},
      "source": [
        "## Context Windows\n",
        "- It would be computationally very expensive and prohibitive to feed entire text into a Transformer all at once. When we actually train a Transformer on large datasets, we only work with chunks of the dataset. During training, we sample random chunks from the training set and train on just these chunks at a time. These chunks have a maximum length called `block_size`.\n",
        "\n",
        "- When we plug data into a Transformer, we simultaneously train it to make predictions at every position. In a chunk of nine characters, there are actually eight individual examples packed in. \n",
        "- We train on all of these not just for computational reasons or efficiency, but also to make the Transformer network accustomed to seeing contexts ranging from as little as one character all the way up to block_size. \n",
        "- This is useful later during inference because while sampling, we can start generation with as little as one character of context, and the Transformer knows how to predict the next character with contexts ranging from one all the way up to block_size. \n",
        "- After block_size, we have to start truncating because the Transformer will never receive more than block_size inputs when predicting the next character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "context-100",
      "metadata": {},
      "outputs": [],
      "source": [
        "block_size = 8  # maximum context length\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1 : block_size + 1]\n",
        "for t in range(block_size):\n",
        "    context = x[: t + 1]\n",
        "    target = y[t]\n",
        "    print(f\"Example {t+1} -> input {context.tolist()} -> target {int(target)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a73bc4",
      "metadata": {},
      "source": [
        "Note, during inference after reaching the block_size, we have to start truncating because the Transformer will never receive more than the `block_size` inputs when it's predicting the next character"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "batching-110",
      "metadata": {},
      "source": [
        "## Mini-batching\n",
        "Sample random starting positions to build batches of shape `(batch_size, block_size)`. Each row is an independent training example. Batching keeps the GPU/CPU busy and speeds up training.\n",
        "> In Transformers, we have many batches of multiple chunks of text that are stacked up in a single tensor. This is done for efficiency to keep the GPUs busy, as they are very good at parallel processing of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "batching-120",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4  # number of sequences processed in parallel\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    source = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(source) - block_size, (batch_size,))\n",
        "    x = torch.stack([source[i : i + block_size] for i in ix])\n",
        "    y = torch.stack([source[i + 1 : i + 1 + block_size] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"xb\", xb.shape)\n",
        "print(\"yb\", yb.shape)\n",
        "# Peek a few (context, target) pairs\n",
        "for xrow, yrow in zip(xb, yb):\n",
        "    for t in range(block_size):\n",
        "        print(f\"input {xrow[:t+1].tolist()} -> target {int(yrow[t])}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-130",
      "metadata": {},
      "source": [
        "## Bigram Model\n",
        "- In language modeling, the Bigram language model is probably the simplest language model\n",
        "- It use a single embedding table of shape `(vocab_size, vocab_size)`. Given a token id, look up a row and interpret it directly as the logits for the next token. \n",
        "    > Embedding Table: This is basically a tensor of shape vocab_size by vocab_size. When we pass indices here, every single integer in our input refers to this embedding table and plucks out a row from that embedding table. We interpret this as the logits, which are the scores for the next character in the sequence.\n",
        "    > What's happening is we're predicting what comes next based solely on the individual identity of a single token. \n",
        "- There’s no context mixing here — it’s a pure bigram model.\n",
        "- Training is done with `cross-entropy loss`.\n",
        "    > Cross entropy loss is negative log likelihood loss. The loss is the cross entropy between the predictions and the targets, which measures the quality of the logits with respect to the targets.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model-140",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super().__init__()\n",
        "        # Each token id maps to a row of logits for the next token\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        # inputs: (B, T) -> logits: (B, T, C) with C=vocab_size\n",
        "        logits = self.token_embedding_table(inputs)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Flatten batch and time for cross-entropy\n",
        "            logits = rearrange(logits, \"b t c -> (b t) c\")\n",
        "            targets = rearrange(targets, \"b t -> (b t)\")\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, inputs, max_new_tokens: int):\n",
        "        # Autoregressively sample `max_new_tokens` tokens\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(inputs)  # (B, T, C)\n",
        "            logits = logits[:, -1, :]  # only last step: (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)  # convert to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            inputs = torch.cat((inputs, idx_next), dim=1)  # append\n",
        "        return inputs\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out, loss = m(xb, yb)\n",
        "print(\"logits shape:\", out.shape, \"| loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1caffd74",
      "metadata": {},
      "source": [
        "We expect the loss to be about -ln(1/65), which is approximately 4.17, but we're getting 4.72. This tells us that the initial predictions are not super diffuse - they have a little bit of structure already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pretrain-sample-150",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Untrained sample (will be gibberish, but shows the flow)\n",
        "start = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(start, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train-160",
      "metadata": {},
      "source": [
        "## Train\n",
        "Use `AdamW` optimizer. A typical good setting for the learning rate is roughly `3e-4`. However, for tiny models we can use a relatively large learning rate (`1e-3`), "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b318585",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-170",
      "metadata": {},
      "outputs": [],
      "source": [
        "for step in range(10000):\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    _, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 200 == 0:\n",
        "        print(f\"step {step:5d} | loss {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sample-180",
      "metadata": {},
      "source": [
        "## Sample\n",
        "Finally, generate text by sampling one token at a time from the model’s predicted distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sample-190",
      "metadata": {},
      "outputs": [],
      "source": [
        "start = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(start, max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d60665e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
