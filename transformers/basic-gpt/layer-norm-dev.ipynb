{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766cb0ee-ee64-4411-9598-84aeccf57efb",
   "metadata": {},
   "source": [
    "## Batch Normalization vs Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671111f5-758e-49eb-a178-50d92226e585",
   "metadata": {},
   "source": [
    "Batch Normalization (taken from karpathy's earlier lectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66a2cf-312c-41da-9de8-d1106d7d837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        # params\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "        # buffers\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)  # batch mean\n",
    "            xvar = x.var(0, keepdim=True)    # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        # normalize to unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "\n",
    "        # scale and shift\n",
    "        out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "print(x.shape)  # [32, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7709bd1-22ab-45a6-8cfb-17b38fbbc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std along batch dimension\n",
    "x[:,0].mean(), x[:,1].std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b09f4f-4b14-4968-8073-9d2e35522b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std along feature dimension\n",
    "x[0,:].mean(), x[1,:].std() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a74c2-c66a-4370-9171-3197ccdc57e7",
   "metadata": {},
   "source": [
    "Layer Normalization\n",
    "- take mean and std across feature dimension\n",
    "- no need to maintain running variance and mean and phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c6c72-707c-4960-9f0e-87f00a24fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LayerNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # params\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # forward pass\n",
    "        xmean = x.mean(1, keepdim=True)  # feature mean\n",
    "        xvar = x.var(1, keepdim=True)    # feature variance\n",
    "\n",
    "        # normalize to unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "\n",
    "        # scale and shift\n",
    "        out = self.gamma * xhat + self.beta\n",
    "\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "print(x.shape)  # [32, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96724e4-ee5d-4fea-a153-2248a7e0b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std along batch dimension\n",
    "x[:,0].mean(), x[:,1].std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb782ce8-fb1c-4a3e-ad91-8e190d2ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std along feature dimension\n",
    "x[0,:].mean(), x[1,:].std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d9880-45a5-42a2-9dae-7dffa9bdd394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
