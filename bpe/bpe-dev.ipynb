{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"नमस्ते 👋 (hello in hindi!)\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ord() returns the Unicode code point for a character\n",
    "[ord(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode() returns the encoded bytes of the string\n",
    "# utf-8 is the most common encoding for text in the world, it is the only encoding that is backwards compatible with ASCII\n",
    "s.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable length (1-4 bytes) utf-8 encodings, here each integer in the list is a byte (decimal representation) in the utf-8 encoding\n",
    "for c in s:\n",
    "    print(c, list(c.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a334960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode('utf-8') # utf-8 encoding of the text (raw bytes)\n",
    "tokens = list(map(int, tokens)) # convert each byte to an integer\n",
    "print('----')\n",
    "print(text)\n",
    "print(len(text))\n",
    "print('----')\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the frequency of each byte pair\n",
    "def get_freqs(ids):\n",
    "    freqs = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    return freqs\n",
    "stats = get_freqs(tokens)\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56fab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf359fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max() finds the key in stats dict that has the highest value\n",
    "# key=stats.get tells max() to compare keys by their corresponding values in the dict\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i<len(ids)-1 and ids[i:i+2] == list(pair):\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "#print(merge([5,6,7,6,5,4,3,2,1,0], (6,7), 99))\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all the text from the link https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.reedbeta.com/blog/programmers-intro-to-unicode/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the main content text (remove scripts, styles, etc.)\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.decompose()\n",
    "\n",
    "# Get text from the main content area\n",
    "text = soup.get_text()\n",
    "\n",
    "# Clean up the text - remove extra whitespace\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "print(f\"Scraped text length: {len(text)} characters\")\n",
    "print(f\"First 500 characters:\\n{text[:500]}...\")\n",
    "\n",
    "# save text to a file\n",
    "with open('data/text.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb85dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.encode('utf-8')\n",
    "tokens = list(map(int, tokens)) \n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the frequency of each byte pair\n",
    "def get_freqs(ids):\n",
    "    freqs = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    return freqs\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i<len(ids)-1 and ids[i:i+2] == list(pair):\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "########################################################\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # make a copy of the tokens\n",
    "# merges will be our trained tokenizer vocabulary\n",
    "merges = {} # (int, int) -> int\n",
    "\n",
    "for i in range(num_merges):\n",
    "    stats = get_freqs(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging {pair} at index {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf86e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get compression ratio\n",
    "print(f\"tokens length: {len(tokens)}\")\n",
    "print(f\"ids length: {len(ids)}\")\n",
    "print(f\"Compression ratio: {len(tokens) / len(ids):.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8af30",
   "metadata": {},
   "source": [
    "Decode the tokens using the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytes([idx]) - creates bytes from a list containing one integer\n",
    "# whereas bytes(idx) - creates a bytes object of length idx, filled with zeros\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given a list of token ids (integers), return the decoded string\n",
    "    # b\"\".join(vocab[id] for id in ids) - concatenates the bytes objects for each id in the list\n",
    "    tokens = b\"\".join(vocab[id] for id in ids)\n",
    "    # replace any invalid utf-8 bytes with the replacement character, as in utf-8 encoding not all bytes are valid (eg. 128)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "print(decode([128]))\n",
    "print(decode([97, 116]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099350a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes(idx) for idx in range(256)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810d370",
   "metadata": {},
   "source": [
    "Encode the text using the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19873af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    # convert the text to a list of tokens\n",
    "    tokens = text.encode('utf-8')\n",
    "    while len(tokens)>=2:\n",
    "        freqs = get_freqs(tokens)\n",
    "        # Find the pair with the lowest merge index (earliest merge in training)\n",
    "        # min() finds the pair with the smallest value returned by the key function\n",
    "        # lambda p: merges.get(p, float('inf')) returns:\n",
    "        #   - the merge index if the pair exists in merges (lower index = earlier merge)\n",
    "        #   - float('inf') if the pair doesn't exist in merges (ensures it won't be selected)\n",
    "        # This ensures we apply merges in the same order they were learned during training\n",
    "        pair = min(freqs, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        tokens = merge(tokens, pair, merges[pair])\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"hello world\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ef9d3",
   "metadata": {},
   "source": [
    "Forced splits using the regex patterns (GPT series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ed5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980bc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
