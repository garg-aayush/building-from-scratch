<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BPE Tokenizer - TL;DR</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }

        h1 {
            text-align: center;
            font-size: 36px;
            margin-bottom: 5px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 18px;
            margin-bottom: 40px;
        }

        .cards-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 10px;
            margin-bottom: 20px;
        }

        .card {
            background: #fff;
            border-radius: 15px;
            padding: 30px;
            border: 2px solid #e0e0e0;
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.15);
        }

        .card-header {
            display: flex;
            align-items: center;
            margin-bottom: 20px;
        }

        .card-number {
            width: 50px;
            height: 50px;
            border-radius: 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 24px;
            font-weight: bold;
            color: white;
            margin-right: 15px;
            flex-shrink: 0;
        }

        .card-title {
            font-size: 24px;
            font-weight: 600;
            color: #333;
            line-height: 1.3;
        }

        .card-content {
            color: #555;
            line-height: 1.6;
        }

        .highlight-box {
            background: #f8f9fa;
            border-left: 4px solid;
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 5px;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }

        .stat-box {
            background: white;
            padding: 15px;
            border-radius: 10px;
            text-align: center;
            border: 1px solid #e0e0e0;
        }

        .stat-value {
            font-size: 28px;
            font-weight: bold;
            margin-bottom: 5px;
        }

        .stat-label {
            font-size: 14px;
            color: #666;
        }

        .card-1 .card-number { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }
        .card-1 .highlight-box { border-left-color: #667eea; }

        .card-2 .card-number { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); }
        .card-2 .highlight-box { border-left-color: #f5576c; }
        .card-2 .stat-value { color: #f5576c; }

        .card-3 .card-number { background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); }
        .card-3 .highlight-box { border-left-color: #4facfe; }
        .card-3 .stat-value { color: #4facfe; }

        .card-4 .card-number { background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%); }
        .card-4 .highlight-box { border-left-color: #43e97b; }
        .card-4 .stat-value { color: #43e97b; }

        .card-5 .card-number { background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); }
        .card-5 .highlight-box { border-left-color: #fa709a; }

        .full-width-card {
            grid-column: 1 / -1;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 14px;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
        }

        th {
            background: #f8f9fa;
            font-weight: 600;
            color: #333;
        }

        .metric-emphasis {
            font-weight: bold;
            color: #f5576c;
        }

        .footer {
            text-align: center;
            margin-top: 5px;
            padding-top: 5px;
            border-top: 2px solid #e0e0e0;
            color: #666;
            font-size: 14px;
        }

        .tech-badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 12px;
            margin: 5px 5px 5px 0;
            font-weight: 500;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Summary: BPE implementation from scratch</h1>

        <div class="cards-grid">
            <!-- Card 1: Training Speed Optimization -->
            <div class="card card-2">
                <div class="card-header">
                    <div class="card-number">1</div>
                    <div class="card-title">Training Speed: 50x Faster</div>
                </div>
                <div class="card-content">
                    <p><strong>Problem:</strong> Baseline was slow - 117s for 20 merges (5.5M chunks, 5-6s/merge)</p>
                    
                    <table>
                        <tr>
                            <th>Metric</th>
                            <th>Baseline</th>
                            <th>Optimized</th>
                            <th>Speedup</th>
                        </tr>
                        <tr>
                            <td>Total time</td>
                            <td>117s</td>
                            <td>2.4s</td>
                            <td class="metric-emphasis">~50x</td>
                        </tr>
                        <tr>
                            <td>Per-merge time</td>
                            <td>5-6s</td>
                            <td>0.017s</td>
                            <td class="metric-emphasis">~300x</td>
                        </tr>
                    </table>

                    <div class="highlight-box" style="margin-top: 15px;">
                        <strong>Key Optimizations:</strong> Frequency-weighted merging with unique pre-tokens + Parallel pre-tokenization (212.8s â†’ 35.2s, <span class="metric-emphasis">~6x faster</span>)
                    </div>
                </div>
            </div>

            <!-- Card 2: Encoding Speed Optimization -->
            <div class="card card-3">
                <div class="card-header">
                    <div class="card-number">2</div>
                    <div class="card-title">Inference Speed: 3.7x Faster</div>
                </div>
                <div class="card-content">
                    <p>Progressive optimization from Python baseline to Rust:</p>
                    
                    <table>
                        <tr>
                            <th>Implementation</th>
                            <th>Time</th>
                            <th>Throughput</th>
                            <th>Speedup</th>
                        </tr>
                        <tr>
                            <td>Python (baseline)</td>
                            <td>21.28s</td>
                            <td>286K tok/s</td>
                            <td>1.0x</td>
                        </tr>
                        <tr>
                            <td>Python (optimized)</td>
                            <td>19.48s</td>
                            <td>313K tok/s</td>
                            <td>1.09x</td>
                        </tr>
                        <tr>
                            <td><strong>Rust (via Python)</strong></td>
                            <td><strong>5.26s</strong></td>
                            <td><strong>1.16M tok/s</strong></td>
                            <td class="metric-emphasis">3.7x</td>
                        </tr>
                    </table>

                    <p style="margin-top: 15px; font-size: 14px; color: #666;">
                        <em>Test: TinyStoriesV2 validation (22.5 MB, 6.09M tokens)</em>
                    </p>
                </div>
            </div>

            <!-- Card 3: Custom Tokenizers -->
            <div class="card card-4">
                <div class="card-header">
                    <div class="card-number">3</div>
                    <div class="card-title">Custom Tokenizers Trained</div>
                </div>
                <div class="card-content">
                    <p>Trained two custom 16K tokenizers on large datasets:</p>
                    
                    <div class="highlight-box">
                        <span class="tech-badge">TinyStoriesV2 (~2.6GB)</span>
                        <span class="tech-badge">FineWeb (~3.3GB)</span>
                    </div>
                    <br>
                    <p style="margin-top: 15px;"><strong>Compression Analysis on Web Text (FineWeb):</strong></p>
                    <table>
                        <tr>
                            <th>Tokenizer</th>
                            <th>Vocab Size</th>
                            <th>Bytes/Token</th>
                        </tr>
                        <tr>
                            <td><strong>GPT-2</strong></td>
                            <td>50K</td>
                            <td class="metric-emphasis">4.64</td>
                        </tr>
                        <tr>
                            <td><strong>fineweb-16K</strong></td>
                            <td>16K</td>
                            <td class="metric-emphasis">4.33</td>
                        </tr>
                        <tr>
                            <td>tinystoriesv2-16K</td>
                            <td>16K</td>
                            <td>3.54</td>
                        </tr>
                    </table>
                    <p style="margin-top: 10px; font-size: 14px; color: #666;">
                        <em>Test: FineWeb evaluation dataset (22.56 MB) - separate from training data</em>
                    </p>
                    <div class="highlight-box" style="margin-top: 10px;">
                        Using a much more diverse dataset like FineWeb achieves a better compression ratio compared to TinyStoriesV2 even with a smaller 16K vocab.
                    </div>
                    <br>
                    <p style="margin-top: 15px;"><strong>Compression on Story Text (TinyStories):</strong></p>
                    <table>
                        <tr>
                            <th>Tokenizer</th>
                            <th>Vocab Size</th>
                            <th>Bytes/Token</th>
                        </tr>
                        <tr>
                            <td><strong>tinystoriesv2-16K</strong></td>
                            <td>16K</td>
                            <td class="metric-emphasis">4.13</td>
                        </tr>
                        <tr>
                            <td>tinystoriesv2-8K</td>
                            <td>8K</td>
                            <td>4.11</td>
                        </tr>
                        <tr>
                            <td>GPT-2</td>
                            <td>50K</td>
                            <td>4.07</td>
                        </tr>
                    </table>
                    <div class="highlight-box" style="margin-top: 10px;">
                        Domain-specific 8K/16K tokenizers beat the general 50K GPT-2 on the target domain. While the practical impact is unclear, this suggests domain-specific tokenizers could provide better compression and performance.
                    </div>
                </div>
            </div>

            <!-- Card 4: GPT-2 Evaluation -->
            <div class="card card-5">
                <div class="card-header">
                    <div class="card-number">4</div>
                    <div class="card-title">GPT-2 Training Evaluation</div>
                </div>
                <div class="card-content">
                    <p>Pre-Trained GPT-2 models (~98M params) to evaluate tokenizer:</p>
                    
                    <img src="images/bpe-comparison-vertical.png" alt="BPE Comparison Results" style="width: 90%; margin: 15px auto; display: block; border-radius: 8px; border: 1px solid #e0e0e0;">

                    <div class="highlight-box" style="margin-top: 15px;">
                        <strong>Conclusion:</strong> FineWeb tokenizer showed marginally better performance. The entire pipeline (training â†’ re-encoding â†’ GPT-2 training) works correctly end-to-end. Longer training or larger models might reveal more significant differences.
                    </div>
                </div>
            </div>
        </div>

        <div class="footer">
            <p> Tokenizer train/opt. on MacBook M3 Pro Max (36GB RAM) <strong>|</strong> GPT-2 training on H100 (80GB) <strong>|</strong> Following Andrej's "Let's build the GPT Tokenizer" with progressive optimizations & exps </p>
        </div>
    </div>
</body>
</html>
