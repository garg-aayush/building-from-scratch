{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e8fea3-9975-43ba-ae7e-1836a8ddba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-05 16:45:08--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘data/input.txt’\n",
      "\n",
      "data/input.txt      100%[===================>]   1.06M  4.20MB/s    in 0.3s    \n",
      "\n",
      "2025-09-05 16:45:09 (4.20 MB/s) - ‘data/input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download and save the data\n",
    "!mkdir -p data && \\\n",
    "    wget -O data/input.txt \\\n",
    "    https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afedb8fc-76cb-46cc-bba1-3b100fda36d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d0256f-ce2d-4f5d-86e8-df0ab752b38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"text length: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3524f76-1e83-4290-9f27-168e94ada25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ec6c2a-e882-44ed-87d5-a9967da7286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# find all the unique characters in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c49b52-faf4-4ecc-847b-84df125d5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-level LM, encoder and decoder\n",
    "# char to int mapping\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# int to char mapping\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encoder: maps the given string to a list of int (tokens)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# decoder: maps the list of int (tokens) to string\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09bfddf1-9d8e-48bb-91eb-ac5c2c082dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 58, 46, 43, 56, 43, 2]\n",
      "Hello, there!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello, there!\"))\n",
    "print(decode(encode(\"Hello, there!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b8383f-eddd-4e8e-8bcd-c92658c67883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# encode the Shakespeare text\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d572ef-3de4-4cc5-bd30-d5011b249ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d7a84d6-50fa-46e0-9a62-96f5a8dd2896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this chunk we have 8 training examples that we train simultaneously\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "417d9ff3-6f71-4e70-b30f-b6ede8e2a0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 -> when input is tensor([18]), target is 47\n",
      "Example 2 -> when input is tensor([18, 47]), target is 56\n",
      "Example 3 -> when input is tensor([18, 47, 56]), target is 57\n",
      "Example 4 -> when input is tensor([18, 47, 56, 57]), target is 58\n",
      "Example 5 -> when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "Example 6 -> when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "Example 7 -> when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "Example 8 -> when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "# this helps with both efficieny and teaching transformers to look at all context length from 1 to block_size, helps in inference\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Example {t+1} -> when input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3405cf2-cb0a-45d5-a964-b3bb20ed9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "Example 1 -> when input is tensor([57]), target is 1\n",
      "Example 2 -> when input is tensor([57,  1]), target is 46\n",
      "Example 3 -> when input is tensor([57,  1, 46]), target is 47\n",
      "Example 4 -> when input is tensor([57,  1, 46, 47]), target is 57\n",
      "Example 5 -> when input is tensor([57,  1, 46, 47, 57]), target is 1\n",
      "Example 6 -> when input is tensor([57,  1, 46, 47, 57,  1]), target is 50\n",
      "Example 7 -> when input is tensor([57,  1, 46, 47, 57,  1, 50]), target is 53\n",
      "Example 8 -> when input is tensor([57,  1, 46, 47, 57,  1, 50, 53]), target is 60\n",
      "---\n",
      "Example 1 -> when input is tensor([1]), target is 58\n",
      "Example 2 -> when input is tensor([ 1, 58]), target is 46\n",
      "Example 3 -> when input is tensor([ 1, 58, 46]), target is 43\n",
      "Example 4 -> when input is tensor([ 1, 58, 46, 43]), target is 56\n",
      "Example 5 -> when input is tensor([ 1, 58, 46, 43, 56]), target is 43\n",
      "Example 6 -> when input is tensor([ 1, 58, 46, 43, 56, 43]), target is 1\n",
      "Example 7 -> when input is tensor([ 1, 58, 46, 43, 56, 43,  1]), target is 41\n",
      "Example 8 -> when input is tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), target is 39\n",
      "---\n",
      "Example 1 -> when input is tensor([17]), target is 26\n",
      "Example 2 -> when input is tensor([17, 26]), target is 15\n",
      "Example 3 -> when input is tensor([17, 26, 15]), target is 17\n",
      "Example 4 -> when input is tensor([17, 26, 15, 17]), target is 10\n",
      "Example 5 -> when input is tensor([17, 26, 15, 17, 10]), target is 0\n",
      "Example 6 -> when input is tensor([17, 26, 15, 17, 10,  0]), target is 32\n",
      "Example 7 -> when input is tensor([17, 26, 15, 17, 10,  0, 32]), target is 53\n",
      "Example 8 -> when input is tensor([17, 26, 15, 17, 10,  0, 32, 53]), target is 1\n",
      "---\n",
      "Example 1 -> when input is tensor([57]), target is 58\n",
      "Example 2 -> when input is tensor([57, 58]), target is 6\n",
      "Example 3 -> when input is tensor([57, 58,  6]), target is 1\n",
      "Example 4 -> when input is tensor([57, 58,  6,  1]), target is 61\n",
      "Example 5 -> when input is tensor([57, 58,  6,  1, 61]), target is 47\n",
      "Example 6 -> when input is tensor([57, 58,  6,  1, 61, 47]), target is 58\n",
      "Example 7 -> when input is tensor([57, 58,  6,  1, 61, 47, 58]), target is 46\n",
      "Example 8 -> when input is tensor([57, 58,  6,  1, 61, 47, 58, 46]), target is 0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4 # num of sequences we will look in parallel\n",
    "block_size = 8 # maximum context for the the predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # get batch_size random indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for x,y in zip(xb,yb):\n",
    "    for t in range(block_size):\n",
    "        context = x[:t+1]\n",
    "        target = y[t]\n",
    "        print(f\"Example {t+1} -> when input is {context}, target is {target}\")\n",
    "    print('---')\n",
    "# here we have 32 independent examples packed in 4 batches of block_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eaca8625-7286-426b-a31f-7b77d93d78a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.8865, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Bigram LM: simplest language model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocal_size):\n",
    "        super().__init__()\n",
    "        # lookup embedding table where we have an embedding row each token in vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        # inputs, targets: B (batch) X T (block_size)\n",
    "        logits = self.token_embedding_table(inputs) # (B, T, C) C: embedding_length\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # targets: (B,T)\n",
    "            logits = rearrange(logits, 'b t c -> (b t) c')\n",
    "            targets = rearrange(targets, 'b t -> (b t)')\n",
    "            loss = F.cross_entropy(logits, targets) # expected value: -ln(1/65)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # inputs: (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(inputs)\n",
    "            # focus only on last token\n",
    "            logits = logits[:,-1,:] # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the sequence\n",
    "            inputs = torch.cat((inputs, idx_next), dim=1) # (B,T+1)\n",
    "        return inputs\n",
    "        \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape, loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9533ca5-f950-4ed6-b096-3d8a19c465a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "o$,q&IWqW&xtCjaB?ij&bYRGkF?b; f ,CbwhtERCIfuWr,DzJERjhLlVaF&EjffPHDFcNoGIG'&$qXisWTkJPw\n",
      " ,b Xgx?D3sj\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec2251b0-81a6-4339-95f4-06cc2b5ca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "# you can get away with large lr for small NN\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45545cfd-01e2-4147-833f-a09d6f6c1877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 -> loss: 2.48880\n",
      "step: 100 -> loss: 2.37263\n",
      "step: 200 -> loss: 2.45426\n",
      "step: 300 -> loss: 2.49152\n",
      "step: 400 -> loss: 2.44427\n",
      "step: 500 -> loss: 2.47666\n",
      "step: 600 -> loss: 2.31586\n",
      "step: 700 -> loss: 2.43365\n",
      "step: 800 -> loss: 2.46397\n",
      "step: 900 -> loss: 2.53290\n",
      "step: 1000 -> loss: 2.52733\n",
      "step: 1100 -> loss: 2.37466\n",
      "step: 1200 -> loss: 2.46733\n",
      "step: 1300 -> loss: 2.43932\n",
      "step: 1400 -> loss: 2.40134\n",
      "step: 1500 -> loss: 2.51769\n",
      "step: 1600 -> loss: 2.41952\n",
      "step: 1700 -> loss: 2.42812\n",
      "step: 1800 -> loss: 2.45226\n",
      "step: 1900 -> loss: 2.48836\n",
      "step: 2000 -> loss: 2.43946\n",
      "step: 2100 -> loss: 2.32969\n",
      "step: 2200 -> loss: 2.34064\n",
      "step: 2300 -> loss: 2.43337\n",
      "step: 2400 -> loss: 2.49782\n",
      "step: 2500 -> loss: 2.56243\n",
      "step: 2600 -> loss: 2.44355\n",
      "step: 2700 -> loss: 2.56188\n",
      "step: 2800 -> loss: 2.44086\n",
      "step: 2900 -> loss: 2.38822\n",
      "step: 3000 -> loss: 2.48613\n",
      "step: 3100 -> loss: 2.46777\n",
      "step: 3200 -> loss: 2.41473\n",
      "step: 3300 -> loss: 2.37576\n",
      "step: 3400 -> loss: 2.36604\n",
      "step: 3500 -> loss: 2.23020\n",
      "step: 3600 -> loss: 2.43888\n",
      "step: 3700 -> loss: 2.42907\n",
      "step: 3800 -> loss: 2.56316\n",
      "step: 3900 -> loss: 2.37115\n",
      "step: 4000 -> loss: 2.48201\n",
      "step: 4100 -> loss: 2.48110\n",
      "step: 4200 -> loss: 2.44069\n",
      "step: 4300 -> loss: 2.43445\n",
      "step: 4400 -> loss: 2.46690\n",
      "step: 4500 -> loss: 2.51266\n",
      "step: 4600 -> loss: 2.41017\n",
      "step: 4700 -> loss: 2.44591\n",
      "step: 4800 -> loss: 2.44600\n",
      "step: 4900 -> loss: 2.48324\n",
      "step: 5000 -> loss: 2.44232\n",
      "step: 5100 -> loss: 2.44925\n",
      "step: 5200 -> loss: 2.37694\n",
      "step: 5300 -> loss: 2.52929\n",
      "step: 5400 -> loss: 2.54358\n",
      "step: 5500 -> loss: 2.42991\n",
      "step: 5600 -> loss: 2.37954\n",
      "step: 5700 -> loss: 2.42825\n",
      "step: 5800 -> loss: 2.47601\n",
      "step: 5900 -> loss: 2.44659\n",
      "step: 6000 -> loss: 2.45037\n",
      "step: 6100 -> loss: 2.43606\n",
      "step: 6200 -> loss: 2.42777\n",
      "step: 6300 -> loss: 2.35874\n",
      "step: 6400 -> loss: 2.39009\n",
      "step: 6500 -> loss: 2.42630\n",
      "step: 6600 -> loss: 2.45718\n",
      "step: 6700 -> loss: 2.42484\n",
      "step: 6800 -> loss: 2.45853\n",
      "step: 6900 -> loss: 2.30946\n",
      "step: 7000 -> loss: 2.34128\n",
      "step: 7100 -> loss: 2.56136\n",
      "step: 7200 -> loss: 2.51822\n",
      "step: 7300 -> loss: 2.51675\n",
      "step: 7400 -> loss: 2.36381\n",
      "step: 7500 -> loss: 2.42283\n",
      "step: 7600 -> loss: 2.50232\n",
      "step: 7700 -> loss: 2.50594\n",
      "step: 7800 -> loss: 2.40026\n",
      "step: 7900 -> loss: 2.32271\n",
      "step: 8000 -> loss: 2.49877\n",
      "step: 8100 -> loss: 2.47075\n",
      "step: 8200 -> loss: 2.50992\n",
      "step: 8300 -> loss: 2.48020\n",
      "step: 8400 -> loss: 2.31658\n",
      "step: 8500 -> loss: 2.45421\n",
      "step: 8600 -> loss: 2.31270\n",
      "step: 8700 -> loss: 2.46306\n",
      "step: 8800 -> loss: 2.30418\n",
      "step: 8900 -> loss: 2.46727\n",
      "step: 9000 -> loss: 2.44716\n",
      "step: 9100 -> loss: 2.48241\n",
      "step: 9200 -> loss: 2.46718\n",
      "step: 9300 -> loss: 2.46533\n",
      "step: 9400 -> loss: 2.46622\n",
      "step: 9500 -> loss: 2.44010\n",
      "step: 9600 -> loss: 2.45124\n",
      "step: 9700 -> loss: 2.50987\n",
      "step: 9800 -> loss: 2.41129\n",
      "step: 9900 -> loss: 2.63404\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps%100 == 0: print(f\"step: {steps} -> loss: {loss.item():.5f}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b7e6a4c-df89-4548-9504-65fd42c15b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hak aifucouthret. gan f, telyonouer'scty\n",
      "Buss ithim nst I y tis s buietrakiswe awere,\n",
      "CEO:\n",
      "Yet:\n",
      "\n",
      "Whe\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0458b5d8-2708-44d6-ad5c-1fe41f00296a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
