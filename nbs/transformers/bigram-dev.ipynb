{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-10",
      "metadata": {},
      "source": [
        "# Bigram Language Model (from scratch)\n",
        "\n",
        "In this notebook, Build and train a tiny character-level bigram language model from [Andrej Karpathy’s \"Let’s build GPT from scratch\"](https://www.youtube.com/watch?v=kCc8FmEb1nY). Keep things simple and self-contained: load Tiny Shakespeare, tokenize at the character level, train a bigram model (each token predicts the next), and sample text.\n",
        "\n",
        "- It is best to follow this notebook with the Andrej's video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-20",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-05 22:45:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘data/input.txt’\n",
            "\n",
            "data/input.txt      100%[===================>]   1.06M  3.76MB/s    in 0.3s    \n",
            "\n",
            "2025-09-05 22:45:19 (3.76 MB/s) - ‘data/input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Tiny Shakespeare once (toy dataset)\n",
        "# Shakespeare - a concatenation of all of Shakespeare's works in a single file.\n",
        "\n",
        "!mkdir -p data && \\\n",
        "    wget -O data/input.txt \\\n",
        "    https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-30",
      "metadata": {},
      "source": [
        "## Load Text\n",
        "Read the entire corpus into memory. It’s small (about 1.1M chars), which is perfect for a quick toy experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "load-40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text length: 1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset\n",
        "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "print(f\"Text length: {len(text)}\")\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocab-50",
      "metadata": {},
      "source": [
        "## Vocabulary and Tokenization\n",
        "Use a simple character-level tokenizer. That means each unique character becomes a token id. It’s crude compared to BPE/sentencepiece, but perfect for understanding the mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2b18d006",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab_size = 65\n"
          ]
        }
      ],
      "source": [
        "# Build the character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(\"vocab_size =\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "vocab-60",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[20, 43, 50, 50, 53, 6, 1, 58, 46, 43, 56, 43, 2]\n",
            "Hello, there!\n"
          ]
        }
      ],
      "source": [
        "# Character-level tokenizer: char <-> id\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]  # string -> list[int]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])  # list[int] -> string\n",
        "\n",
        "print(encode(\"Hello, there!\"))\n",
        "print(decode(encode(\"Hello, there!\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numericalize-70",
      "metadata": {},
      "source": [
        "## Numericalize and Split\n",
        "Convert the full corpus to token ids (a long 1D tensor), then keep 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d1716ab2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "numericalize-80",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/val split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "context-90",
      "metadata": {},
      "source": [
        "## Context Windows\n",
        "- It would be computationally very expensive and prohibitive to feed entire text into a Transformer all at once. When we actually train a Transformer on large datasets, we only work with chunks of the dataset. During training, we sample random chunks from the training set and train on just these chunks at a time. These chunks have a maximum length called `block_size`.\n",
        "\n",
        "- When we plug data into a Transformer, we simultaneously train it to make predictions at every position. In a chunk of nine characters, there are actually eight individual examples packed in. \n",
        "- We train on all of these not just for computational reasons or efficiency, but also to make the Transformer network accustomed to seeing contexts ranging from as little as one character all the way up to block_size. \n",
        "- This is useful later during inference because while sampling, we can start generation with as little as one character of context, and the Transformer knows how to predict the next character with contexts ranging from one all the way up to block_size. \n",
        "- After block_size, we have to start truncating because the Transformer will never receive more than block_size inputs when predicting the next character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "context-100",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 -> input [18] -> target 47\n",
            "Example 2 -> input [18, 47] -> target 56\n",
            "Example 3 -> input [18, 47, 56] -> target 57\n",
            "Example 4 -> input [18, 47, 56, 57] -> target 58\n",
            "Example 5 -> input [18, 47, 56, 57, 58] -> target 1\n",
            "Example 6 -> input [18, 47, 56, 57, 58, 1] -> target 15\n",
            "Example 7 -> input [18, 47, 56, 57, 58, 1, 15] -> target 47\n",
            "Example 8 -> input [18, 47, 56, 57, 58, 1, 15, 47] -> target 58\n"
          ]
        }
      ],
      "source": [
        "block_size = 8  # maximum context length\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1 : block_size + 1]\n",
        "for t in range(block_size):\n",
        "    context = x[: t + 1]\n",
        "    target = y[t]\n",
        "    print(f\"Example {t+1} -> input {context.tolist()} -> target {int(target)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a73bc4",
      "metadata": {},
      "source": [
        "Note, during inference after reaching the block_size, we have to start truncating because the Transformer will never receive more than the `block_size` inputs when it's predicting the next character"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "batching-110",
      "metadata": {},
      "source": [
        "## Mini-batching\n",
        "Sample random starting positions to build batches of shape `(batch_size, block_size)`. Each row is an independent training example. Batching keeps the GPU/CPU busy and speeds up training.\n",
        "> In Transformers, we have many batches of multiple chunks of text that are stacked up in a single tensor. This is done for efficiency to keep the GPUs busy, as they are very good at parallel processing of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "batching-120",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xb torch.Size([4, 8])\n",
            "yb torch.Size([4, 8])\n",
            "input [57] -> target 1\n",
            "input [57, 1] -> target 46\n",
            "input [57, 1, 46] -> target 47\n",
            "input [57, 1, 46, 47] -> target 57\n",
            "input [57, 1, 46, 47, 57] -> target 1\n",
            "input [57, 1, 46, 47, 57, 1] -> target 50\n",
            "input [57, 1, 46, 47, 57, 1, 50] -> target 53\n",
            "input [57, 1, 46, 47, 57, 1, 50, 53] -> target 60\n",
            "---\n",
            "input [1] -> target 58\n",
            "input [1, 58] -> target 46\n",
            "input [1, 58, 46] -> target 43\n",
            "input [1, 58, 46, 43] -> target 56\n",
            "input [1, 58, 46, 43, 56] -> target 43\n",
            "input [1, 58, 46, 43, 56, 43] -> target 1\n",
            "input [1, 58, 46, 43, 56, 43, 1] -> target 41\n",
            "input [1, 58, 46, 43, 56, 43, 1, 41] -> target 39\n",
            "---\n",
            "input [17] -> target 26\n",
            "input [17, 26] -> target 15\n",
            "input [17, 26, 15] -> target 17\n",
            "input [17, 26, 15, 17] -> target 10\n",
            "input [17, 26, 15, 17, 10] -> target 0\n",
            "input [17, 26, 15, 17, 10, 0] -> target 32\n",
            "input [17, 26, 15, 17, 10, 0, 32] -> target 53\n",
            "input [17, 26, 15, 17, 10, 0, 32, 53] -> target 1\n",
            "---\n",
            "input [57] -> target 58\n",
            "input [57, 58] -> target 6\n",
            "input [57, 58, 6] -> target 1\n",
            "input [57, 58, 6, 1] -> target 61\n",
            "input [57, 58, 6, 1, 61] -> target 47\n",
            "input [57, 58, 6, 1, 61, 47] -> target 58\n",
            "input [57, 58, 6, 1, 61, 47, 58] -> target 46\n",
            "input [57, 58, 6, 1, 61, 47, 58, 46] -> target 0\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4  # number of sequences processed in parallel\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    source = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(source) - block_size, (batch_size,))\n",
        "    x = torch.stack([source[i : i + block_size] for i in ix])\n",
        "    y = torch.stack([source[i + 1 : i + 1 + block_size] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"xb\", xb.shape)\n",
        "print(\"yb\", yb.shape)\n",
        "# Peek a few (context, target) pairs\n",
        "for xrow, yrow in zip(xb, yb):\n",
        "    for t in range(block_size):\n",
        "        print(f\"input {xrow[:t+1].tolist()} -> target {int(yrow[t])}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-130",
      "metadata": {},
      "source": [
        "## Bigram Model\n",
        "- In language modeling, the Bigram language model is probably the simplest language model\n",
        "- It use a single embedding table of shape `(vocab_size, vocab_size)`. Given a token id, look up a row and interpret it directly as the logits for the next token. \n",
        "    > Embedding Table: This is basically a tensor of shape vocab_size by vocab_size. When we pass indices here, every single integer in our input refers to this embedding table and plucks out a row from that embedding table. We interpret this as the logits, which are the scores for the next character in the sequence.\n",
        "    > What's happening is we're predicting what comes next based solely on the individual identity of a single token. \n",
        "- There’s no context mixing here — it’s a pure bigram model.\n",
        "- Training is done with `cross-entropy loss`.\n",
        "    > Cross entropy loss is negative log likelihood loss. The loss is the cross entropy between the predictions and the targets, which measures the quality of the logits with respect to the targets.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "model-140",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits shape: torch.Size([32, 65]) | loss: 4.724130630493164\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super().__init__()\n",
        "        # Each token id maps to a row of logits for the next token\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        # inputs: (B, T) -> logits: (B, T, C) with C=vocab_size\n",
        "        logits = self.token_embedding_table(inputs)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Flatten batch and time for cross-entropy\n",
        "            logits = rearrange(logits, \"b t c -> (b t) c\")\n",
        "            targets = rearrange(targets, \"b t -> (b t)\")\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, inputs, max_new_tokens: int):\n",
        "        # Autoregressively sample `max_new_tokens` tokens\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(inputs)  # (B, T, C)\n",
        "            logits = logits[:, -1, :]  # only last step: (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)  # convert to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            inputs = torch.cat((inputs, idx_next), dim=1)  # append\n",
        "        return inputs\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out, loss = m(xb, yb)\n",
        "print(\"logits shape:\", out.shape, \"| loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1caffd74",
      "metadata": {},
      "source": [
        "We expect the loss to be about -ln(1/65), which is approximately 4.17, but we're getting 4.72. This tells us that the initial predictions are not super diffuse - they have a little bit of structure already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "pretrain-sample-150",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "cfYCDRUZsYBsA?Y?vgB!ZWOEiAoezL:q&Avufr?gSGdWrp&Bxt-R?wo'TYhBChdIC-RDaRmEGENyouVg'UjyQNyQSpZUVeN:BZqh\n"
          ]
        }
      ],
      "source": [
        "# Untrained sample (will be gibberish, but shows the flow)\n",
        "start = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(start, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train-160",
      "metadata": {},
      "source": [
        "## Train\n",
        "Use `AdamW` optimizer. A typical good setting for the learning rate is roughly `3e-4`. However, for tiny models we can use a relatively large learning rate (`1e-3`), "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3b318585",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "train-170",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step     0 | loss 4.7358\n",
            "step   200 | loss 4.5046\n",
            "step   400 | loss 4.2356\n",
            "step   600 | loss 4.0194\n",
            "step   800 | loss 3.8142\n",
            "step  1000 | loss 3.6808\n",
            "step  1200 | loss 3.5124\n",
            "step  1400 | loss 3.4114\n",
            "step  1600 | loss 3.3483\n",
            "step  1800 | loss 3.1592\n",
            "step  2000 | loss 3.1517\n",
            "step  2200 | loss 2.9603\n",
            "step  2400 | loss 3.0527\n",
            "step  2600 | loss 2.9030\n",
            "step  2800 | loss 2.7741\n",
            "step  3000 | loss 2.7933\n",
            "step  3200 | loss 2.7562\n",
            "step  3400 | loss 2.6762\n",
            "step  3600 | loss 2.7054\n",
            "step  3800 | loss 2.7220\n",
            "step  4000 | loss 2.6779\n",
            "step  4200 | loss 2.5870\n",
            "step  4400 | loss 2.5370\n",
            "step  4600 | loss 2.5282\n",
            "step  4800 | loss 2.6146\n",
            "step  5000 | loss 2.5610\n",
            "step  5200 | loss 2.5641\n",
            "step  5400 | loss 2.6263\n",
            "step  5600 | loss 2.5751\n",
            "step  5800 | loss 2.5393\n",
            "step  6000 | loss 2.4923\n",
            "step  6200 | loss 2.6814\n",
            "step  6400 | loss 2.4553\n",
            "step  6600 | loss 2.4046\n",
            "step  6800 | loss 2.5248\n",
            "step  7000 | loss 2.5210\n",
            "step  7200 | loss 2.4300\n",
            "step  7400 | loss 2.2994\n",
            "step  7600 | loss 2.5724\n",
            "step  7800 | loss 2.5194\n",
            "step  8000 | loss 2.4885\n",
            "step  8200 | loss 2.4565\n",
            "step  8400 | loss 2.3821\n",
            "step  8600 | loss 2.3839\n",
            "step  8800 | loss 2.5126\n",
            "step  9000 | loss 2.4587\n",
            "step  9200 | loss 2.4858\n",
            "step  9400 | loss 2.5564\n",
            "step  9600 | loss 2.4343\n",
            "step  9800 | loss 2.5294\n"
          ]
        }
      ],
      "source": [
        "for step in range(10000):\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    _, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 200 == 0:\n",
        "        print(f\"step {step:5d} | loss {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sample-180",
      "metadata": {},
      "source": [
        "## Sample\n",
        "Finally, generate text by sampling one token at a time from the model’s predicted distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "sample-190",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ORThay garomy CHA hthe IZ:\n",
            "Then'swilisteat\n",
            "HERI w bakee ckinthy\n",
            "S:\n",
            "CIf ditomaisonute ghilyow bloker twiartofre.\n",
            "HE:\n",
            "nse,\n",
            "CII'l t Sthenearoor t ickorathisithet,\n",
            "ICEvel hilichos thton.\n",
            "AMat y thiseefos I acqust mee sprane id\n",
            "TAMisheedeatof ss,\n",
            "Mowiler anes th ct f aiorthranins f tur;\n",
            "tes thy, cit.\n",
            "Sts\n"
          ]
        }
      ],
      "source": [
        "start = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(start, max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d60665e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
